{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake game logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T18:24:24.552022Z",
     "iopub.status.busy": "2023-11-18T18:24:24.551737Z",
     "iopub.status.idle": "2023-11-18T18:24:24.567078Z",
     "shell.execute_reply": "2023-11-18T18:24:24.566203Z",
     "shell.execute_reply.started": "2023-11-18T18:24:24.551996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self, size=N):\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake = [(self.size // 2, self.size // 2)]\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.game_over = False\n",
    "\n",
    "    def _place_food(self):\n",
    "        while self.food is None or self.food in self.snake:\n",
    "            self.food = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.game_over:\n",
    "            return self.obs, self.score, self.game_over, {}\n",
    "\n",
    "        # Directions: 0-Up, 1-Down, 2-Right,  3-Left\n",
    "        direction = [(0, -1), (0, 1), (1, 0),  (-1, 0)][action]\n",
    "        new_head = (self.snake[0][0] + direction[0], self.snake[0][1] + direction[1])\n",
    "\n",
    "        # Check for game over conditions\n",
    "        if (new_head in self.snake) or new_head[0] < 0 or new_head[0] >= self.size or new_head[1] < 0 or new_head[1] >= self.size:\n",
    "            self.game_over = True\n",
    "            return self.obs, self.score, self.game_over, {}\n",
    "\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        # Check if snake eats food\n",
    "        if new_head == self.food:\n",
    "            self.score += 1\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "\n",
    "        self.obs = self.get_observation()\n",
    "        return self.obs, self.score, self.game_over, {}\n",
    "\n",
    "    def get_observation(self):\n",
    "        obs = np.zeros((self.size, self.size))\n",
    "        for x, y in self.snake:\n",
    "            obs[y,x] = 1\n",
    "        x, y = self.food\n",
    "        obs[y,x] = 2\n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        obs = self.get_observation()\n",
    "        for line in obs :\n",
    "            print(line, end=\"\\n\")\n",
    "        print()\n",
    "        \n",
    "    def quit(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cce7e58-724b-456d-986d-69cbb5c0c5fe",
    "_uuid": "b36b2165-4c05-4550-b359-e746a5545e1b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-18T18:24:24.568763Z",
     "iopub.status.busy": "2023-11-18T18:24:24.568424Z",
     "iopub.status.idle": "2023-11-18T18:24:24.888057Z",
     "shell.execute_reply": "2023-11-18T18:24:24.887149Z",
     "shell.execute_reply.started": "2023-11-18T18:24:24.568732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class SnakeEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    step(action): This method takes an action as input, updates the game state based on that action, returns the new state, the reward gained (or lost), whether the game is over (done), and additional info if necessary.\n",
    "    reset(): This method resets the environment to an initial state and returns this initial state. It's used at the beginning of a new episode.\n",
    "    render(): This method is for visualizing the state of the environment. Depending on how you want to view the game, this could simply update the game window.\n",
    "    close(): This method performs any necessary cleanup, like closing the game window.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SnakeEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(4) # Output\n",
    "        self.observation_space = spaces.Box(low=0, high=2,\n",
    "                                            shape=(1, N, N), dtype=np.int32)\n",
    "        self.snake_game = None\n",
    "        self.previous_score = 0\n",
    "        self.last_distance = np.inf\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "        \n",
    "    def seed(self, seed=42): # needed with make_vec_env\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def euclidean_distance_centroid(self, obs):\n",
    "        snake_positions = np.argwhere(obs == 1)\n",
    "        food_positions = np.argwhere(obs == 2)\n",
    "        snake_centroid = np.mean(snake_positions, axis=0)\n",
    "        food_position = food_positions[0]  \n",
    "\n",
    "        new_distance = np.linalg.norm(snake_centroid - food_position)\n",
    "        return new_distance\n",
    "    \n",
    "\n",
    "    def feature_gen_euclidean_distance_to_food(self, raw_obs):\n",
    "        n = raw_obs.shape[0]\n",
    "        snake_positions = np.argwhere(raw_obs == 2)\n",
    "        point_coords = snake_positions[0].astype(float)\n",
    "        \n",
    "        x_coords, y_coords = np.meshgrid(np.arange(raw_obs.shape[0]), np.arange(raw_obs.shape[1]))\n",
    "        obs = np.sqrt((x_coords - point_coords[0])**2 + (y_coords - point_coords[1])**2)\n",
    "        return obs.reshape((1, N, N))\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_obs, score, done, _ = self.snake_game.step(action)\n",
    "\n",
    "        # Calculate the Euclidean distance between the snake and the food\n",
    "        new_distance = self.euclidean_distance_centroid(raw_obs)\n",
    "\n",
    "        # Check if the snake has eaten food and update the reward\n",
    "        if self.previous_score != score:\n",
    "            reward = 100\n",
    "            self.previous_score = score\n",
    "        elif done:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward =  1/10 if new_distance < self.last_distance else -1/100\n",
    "        self.last_distance = new_distance\n",
    "        \n",
    "        obs = self.feature_gen_euclidean_distance_to_food(raw_obs)\n",
    "        return obs, reward, done, _\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake_game = SnakeGame()\n",
    "        return self.snake_game.get_observation()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            self.snake_game.render()\n",
    "            \n",
    "    def close(self):\n",
    "        self.snake_game.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T18:24:24.890711Z",
     "iopub.status.busy": "2023-11-18T18:24:24.890421Z",
     "iopub.status.idle": "2023-11-18T18:24:39.231026Z",
     "shell.execute_reply": "2023-11-18T18:24:39.230141Z",
     "shell.execute_reply.started": "2023-11-18T18:24:24.890686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "# The make_vec_env function from Stable Baselines 3 is used to create vectorized environments. \n",
    "# Vectorized environments allow you to run multiple instances of an environment in parallel, \n",
    "# providing a more efficient way to collect experiences (states, actions, rewards, etc.) during training.\n",
    "\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \n",
    "    def __init__(self, observation_space, features_dim: int=128):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.sample().shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "        \n",
    "        # n_flatten = (N-Conv2d*kernel_size)**2\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    # Default policy of PPO (stable_baseline3?)=> take the `action_space` of SnakeEnv for final layer and a softmax activation\n",
    "    def forward(self, observations):\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "    \n",
    "def evaluate_model(model, eval_env, num_episodes=10):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"evaluation {episode=}\")\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        for _ in range(1000):\n",
    "            action, _states = model.predict(np.reshape(obs, (1, N, N)), deterministic=True)\n",
    "            obs, reward, done, _ = eval_env.step(action)\n",
    "            total_rewards += reward\n",
    "            if done :\n",
    "                break\n",
    "            \n",
    "        all_rewards.append(total_rewards)\n",
    "    average_reward = sum(all_rewards) / num_episodes\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T18:32:15.334441Z",
     "iopub.status.busy": "2023-11-18T18:32:15.333764Z",
     "iopub.status.idle": "2023-11-18T18:38:04.678077Z",
     "shell.execute_reply": "2023-11-18T18:38:04.677163Z",
     "shell.execute_reply.started": "2023-11-18T18:32:15.334408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time   \n",
    "from stable_baselines3 import PPO,DQN, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "train_env = make_vec_env(lambda: SnakeEnv(), n_envs=2)\n",
    "eval_env = SnakeEnv()\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=LinearQNet,\n",
    ")\n",
    "\n",
    "learning_rate_schedule = get_schedule_fn(0.0003)\n",
    "model = PPO(\"CnnPolicy\", train_env, policy_kwargs=policy_kwargs, verbose=2,learning_rate=learning_rate_schedule)               \n",
    "\n",
    "# new_logger = configure(\"path_to_save_logs\", [\"stdout\", \"tensorboard\"])\n",
    "# model.set_logger(new_logger) # Run TensorBoard in a terminal: tensorboard --logdir=path_to_save_logs\n",
    "\n",
    "\n",
    "total_timesteps = 200_000\n",
    "eval_interval = 50_000  \n",
    "num_eval_episodes = 100  \n",
    "\n",
    "# Training loop with periodic evaluation\n",
    "for _ in range(0, total_timesteps, eval_interval):\n",
    "    model.learn(total_timesteps=eval_interval)\n",
    "    avg_reward = evaluate_model(model, eval_env, num_episodes=10)\n",
    "    print(f\"Evaluation average reward: {avg_reward}\")\n",
    "\n",
    "model.save(\"ppo_snake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T19:02:30.933461Z",
     "iopub.status.busy": "2023-11-18T19:02:30.932736Z",
     "iopub.status.idle": "2023-11-18T19:02:47.407731Z",
     "shell.execute_reply": "2023-11-18T19:02:47.406735Z",
     "shell.execute_reply.started": "2023-11-18T19:02:30.933423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO \n",
    "\n",
    "\n",
    "env = SnakeEnv()\n",
    "# model = PPO.load(\"ppo_snake\", env=env)\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "env.render()\n",
    "while not done:\n",
    "    input(\"press enter to continue\")\n",
    "    action, _info = model.predict(np.reshape(obs, (1, N, N)), deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    #input(\"press key for next step\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
