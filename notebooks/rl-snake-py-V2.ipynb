{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake game logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_game import FastSnakeGame, SnakeGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DML OK: privateuseone:0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "try:\n",
    "    import torch_directml\n",
    "    dml = torch_directml.device()\n",
    "    x = torch.randn(2,2, device=dml)\n",
    "    print(\"DML OK:\", x.device)\n",
    "except Exception as e:\n",
    "    print(\"DML failed:\", e, file=sys.stderr)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cce7e58-724b-456d-986d-69cbb5c0c5fe",
    "_uuid": "b36b2165-4c05-4550-b359-e746a5545e1b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-01T16:15:27.021300Z",
     "iopub.status.busy": "2023-12-01T16:15:27.020774Z",
     "iopub.status.idle": "2023-12-01T16:15:27.038928Z",
     "shell.execute_reply": "2023-12-01T16:15:27.037973Z",
     "shell.execute_reply.started": "2023-12-01T16:15:27.021261Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "OBS_SHAPE = 6\n",
    "SHAPE = (OBS_SHAPE, )\n",
    "\n",
    "class SnakeEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    step(action): This method takes an action as input, updates the game state based on that action, returns the new state, the reward gained (or lost), whether the game is over (done), and additional info if necessary.\n",
    "    reset(): This method resets the environment to an initial state and returns this initial state. It's used at the beginning of a new episode.\n",
    "    render(): This method is for visualizing the state of the environment. Depending on how you want to view the game, this could simply update the game window.\n",
    "    close(): This method performs any necessary cleanup, like closing the game window.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game_size:int=0, fast_game:bool=True):\n",
    "        super(SnakeEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(4) # Output\n",
    "        self.observation_space = gym.spaces.Box(low=-4, high=5, shape=SHAPE, dtype=np.float64)\n",
    "        self.render_mode = \"human\"  # Default render mode\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = {\"renders_mode\":[\"human\"]}\n",
    "        self.game_size = game_size\n",
    "        self.SnakeGameHandler = SnakeGame if not fast_game else FastSnakeGame\n",
    "        self._init()    \n",
    "    \n",
    "    def _init(self):\n",
    "        self.snake_game = self.SnakeGameHandler(self.game_size)\n",
    "        food = np.array(self.snake_game.food)\n",
    "        head = np.array(self.snake_game.snake[0])\n",
    "        self._last_distance = self.euclidean_distance(head=head, food=food)\n",
    "        self.previous_score = 0\n",
    "        \n",
    "        # define during run\n",
    "        self.food_position = None\n",
    "        self.snake_positions = None\n",
    "        self.angle_to_food = None\n",
    "        \n",
    "    def seed(self, seed=42): # needed with make_vec_env\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_snake_and_food_position(self, raw_obs):\n",
    "        self.food_position = np.argwhere(raw_obs == 2).flatten()\n",
    "        self.snake_positions = np.argwhere(raw_obs == 1).flatten()\n",
    "        \n",
    "    def euclidean_distance(self, head=None, food=None):\n",
    "        \"\"\" Calculate the Euclidean distance between the centroid of the snake and the food position.\"\"\"\n",
    "        head = head if head is not None else self.snake_positions.take((0, 1))\n",
    "        food = food if food is not None else self.food_position\n",
    "        new_distance = np.linalg.norm(head - food)\n",
    "        return new_distance/self.game_size  # Normalize by the size of the game grid\n",
    "\n",
    "    def get_neighbors(self, grid, head, out_of_bounds_value=3):\n",
    "        \"\"\"Utilise le padding pour gérer les bordures\"\"\"\n",
    "        # Ajouter un padding de 1 avec la valeur out_of_bounds\n",
    "        padded = np.pad(grid, 1, mode='constant', constant_values=out_of_bounds_value)\n",
    "        # Ajuster les indices pour le tableau paddé\n",
    "        i, j = head\n",
    "        pi, pj = i + 1, j + 1\n",
    "        # Récupérer les voisins\n",
    "        neighbors = np.array([\n",
    "            padded[pi-1, pj],  # haut\n",
    "            padded[pi, pj-1],  # gauche\n",
    "            padded[pi+1, pj],  # bas\n",
    "            padded[pi, pj+1],  # droite\n",
    "        ])\n",
    "        return neighbors\n",
    "\n",
    "    def angle_between_snake_head_and_food(self, head, food):\n",
    "        \"\"\"Calculate the angle between the snake's head and the food.\"\"\"\n",
    "        delta_x = food[0] - head[0]\n",
    "        delta_y = food[1] - head[1]\n",
    "        self.angle = np.array([np.arctan2(delta_y, delta_x)])\n",
    "        return self.angle\n",
    "    \n",
    "    def feature_gen(self, raw_obs):\n",
    "        self.set_snake_and_food_position(raw_obs)\n",
    "        new_distance = self.euclidean_distance()\n",
    "        distance = np.array([new_distance])\n",
    "        \n",
    "        head = self.snake_positions.take((0, 1))\n",
    "        # recuper les voisins direct de la tete du serpent\n",
    "        neighbors = self.get_neighbors(raw_obs, head) \n",
    "        angle = self.angle_between_snake_head_and_food(head, self.food_position) \n",
    "\n",
    "        obs = np.concatenate([neighbors, distance, angle])\n",
    "        return obs\n",
    "        \n",
    "    def get_reward(self, score, done):\n",
    "        # Calculate the Euclidean distance between the snake and the food\n",
    "        new_distance = self.euclidean_distance()\n",
    "        # Check if the snake has eaten food and update the reward\n",
    "        if self.previous_score != score:\n",
    "            reward = 10\n",
    "            self.previous_score = score\n",
    "        elif done:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward =  1 if new_distance <= self._last_distance else -1\n",
    "        self._last_distance = new_distance\n",
    "        return reward\n",
    "        \n",
    "    @property\n",
    "    def obs(self):\n",
    "        raw_obs = self.snake_game.raw_obs\n",
    "        return self.feature_gen(raw_obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_obs, score, done, _ = self.snake_game.step(action)\n",
    "        obs = self.feature_gen(raw_obs)\n",
    "        reward = self.get_reward(score, done)\n",
    "        terminated = done  # done = self.snake_game.game_over\n",
    "        truncated = False  # In this case, we don't have a time limit, so no\n",
    "        return obs, reward, terminated, truncated, _\n",
    "\n",
    "    def reset(self, seed=42):\n",
    "        self.seed(seed)\n",
    "        self._init()\n",
    "        return self.obs, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if mode == \"human\":\n",
    "            self.snake_game.render()\n",
    "            \n",
    "    def close(self):\n",
    "        self.snake_game.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T15:11:12.530442Z",
     "iopub.status.busy": "2023-12-01T15:11:12.530072Z",
     "iopub.status.idle": "2023-12-01T15:11:12.535015Z",
     "shell.execute_reply": "2023-12-01T15:11:12.534121Z",
     "shell.execute_reply.started": "2023-12-01T15:11:12.530417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = SnakeEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T15:57:13.726736Z",
     "iopub.status.busy": "2023-12-01T15:57:13.726359Z",
     "iopub.status.idle": "2023-12-01T15:57:13.737447Z",
     "shell.execute_reply": "2023-12-01T15:57:13.736487Z",
     "shell.execute_reply.started": "2023-12-01T15:57:13.726703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        self.cnn = nn.Sequential(\n",
    "                nn.Conv2d(1, features_dim//2, kernel_size=3, stride=1),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(features_dim//2, features_dim, kernel_size=3, stride=1),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(features_dim, features_dim, kernel_size=3, stride=1),\n",
    "                nn.MaxPool2d(),\n",
    "                nn.Conv2d(features_dim, features_dim, kernel_size=3, stride=1),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.AdaptiveMaxPool2d((4,4)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(features_dim * 4 * 4, features_dim),  # Adjust input\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.cnn(X)\n",
    "        return out\n",
    "\n",
    "# Feature extractor for the Snake environment, no softmax or activation function\n",
    "class LinearQNet(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=32):\n",
    "        super(LinearQNet, self).__init__(observation_space, features_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.flatten(\n",
    "                torch.as_tensor(observation_space.sample()[None]).float() # expand dims for batch size\n",
    "            ).shape[1]\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "                nn.Linear(n_flatten, features_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(features_dim, features_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(features_dim, features_dim),\n",
    "                nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        flat = self.flatten(X)\n",
    "        out = self.linear(flat)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def evaluate_model(model, eval_env, num_episodes=10):\n",
    "    all_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        # Si l'environnement retourne un tuple (obs, info), extraire obs\n",
    "        if isinstance(obs, tuple):\n",
    "            obs = obs[0]\n",
    "        \n",
    "        terminated = False\n",
    "        total_rewards = 0\n",
    "        for _ in range(1000):\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            step_result = eval_env.step(action)\n",
    "            \n",
    "            # Gérer les différents formats de retour (gym vs gymnasium)\n",
    "            if len(step_result) == 5:  # Format gymnasium: obs, reward, terminated, truncated, info\n",
    "                obs, reward, terminated, truncated, info = step_result\n",
    "                terminated = terminated or truncated\n",
    "            else:  # Format gym: obs, reward, done, info\n",
    "                obs, reward, terminated, info = step_result\n",
    "                \n",
    "            total_rewards += reward\n",
    "            if all(terminated):\n",
    "                break\n",
    "            \n",
    "        all_rewards.append(total_rewards)\n",
    "    average_reward = sum(all_rewards) / num_episodes\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9491, 0.0000, 0.0000, 0.0593, 0.0000, 0.3417, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.1041, 0.0000, 0.2752, 0.0000, 0.1235, 0.3380,\n",
       "         0.0000, 0.4349, 0.0000, 0.0000, 0.2786, 0.0000, 0.0000, 0.4551, 0.0000,\n",
       "         0.1392, 0.0339, 0.2765, 0.3157, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env = SnakeEnv()\n",
    "model2 = LinearQNet(eval_env.observation_space)\n",
    "model2.forward(torch.as_tensor(eval_env.observation_space.sample()[None]).float())  # Add batch dimension for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The make_vec_env function from Stable Baselines 3 is used to create vectorized environments. <br>\n",
    "Vectorized environments allow you to run multiple instances of an environment in parallel, <br>\n",
    "providing a more efficient way to collect experiences (states, actions, rewards, etc.) during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,DQN, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from stable_baselines3.common.utils import get_schedule_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(n_envs:int=5, use_frame_stack:bool=False, n_stack:int=4, game_size:int=N):\n",
    "    # make_vec_env handle the multiprocessing details\n",
    "    env = make_vec_env(\n",
    "        lambda: SnakeEnv(game_size=game_size), \n",
    "        n_envs=n_envs,  \n",
    "        seed=42    \n",
    "    )\n",
    "    if use_frame_stack:\n",
    "        env = VecFrameStack(env, n_stack=n_stack, channels_order='first')\n",
    "    return env\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model_name:str, \n",
    "                policy_kwargs=None, \n",
    "                game_size:int=30, \n",
    "                n_envs:int=5, \n",
    "                n_stack:int=4, use_frame_stack:bool=False,\n",
    "                verbose:int=2,\n",
    "                ):\n",
    "        self.model_name = model_name\n",
    "        self.policy_kwargs = policy_kwargs\n",
    "        self.game_size = game_size\n",
    "        self.n_envs = n_envs\n",
    "        self.n_stack = n_stack\n",
    "        self.use_frame_stack = use_frame_stack\n",
    "        self.verbose = verbose\n",
    "        self.train_env = get_env(use_frame_stack=self.use_frame_stack, n_envs=n_envs, n_stack=n_stack, game_size=game_size)\n",
    "        self.model = self.get_model(self.model_name, policy_kwargs=self.policy_kwargs)\n",
    "    \n",
    "    def get_model(self, model_name, policy_kwargs=None):\n",
    "        if model_name == \"PPO\":\n",
    "            # MlpPolicy for vectore base and CnnPolicy for image base\n",
    "            model = PPO(\"MlpPolicy\", self.train_env,\n",
    "                        policy_kwargs=policy_kwargs, \n",
    "                        verbose=self.verbose,\n",
    "                        learning_rate=get_schedule_fn(0.0003), \n",
    "                        n_steps=100)               \n",
    "        elif model_name == \"DQN\":\n",
    "            model = DQN(\"MlpPolicy\", self.train_env, \n",
    "                        policy_kwargs=policy_kwargs,  # Enable custom features extractor\n",
    "                        verbose=self.verbose,\n",
    "                        learning_rate=1e-3,            # Fixed learning rate (no schedule needed for DQN)\n",
    "                        buffer_size=10000,             # Size of replay buffer\n",
    "                        learning_starts=1000,          # Start learning after this many steps\n",
    "                        target_update_interval=500,    # Update target network every 500 steps\n",
    "                        train_freq=4,                  # Train every 4 steps\n",
    "                        gradient_steps=1,              # Number of gradient steps per training\n",
    "                        exploration_fraction=0.3,      # Fraction of training for exploration\n",
    "                        exploration_initial_eps=1.0,   # Initial exploration probability\n",
    "                        exploration_final_eps=0.05)    # Final exploration probability\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} is not supported.\")\n",
    "        return model\n",
    "    \n",
    "    def train(self, multiplicator:float=10):\n",
    "        new_logger = configure(\"save_logs\", [\"stdout\", \"tensorboard\"])\n",
    "        self.model.set_logger(new_logger) # Run TensorBoard in a terminal: tensorboard --logdir=save_logs\n",
    "        # IMPORTANT: Utiliser le même type d'environnement pour l'évaluation que pour l'entraînement\n",
    "        eval_env = get_env(use_frame_stack=self.use_frame_stack, game_size=self.game_size, n_stack=self.n_stack, n_envs=self.n_envs)\n",
    "\n",
    "        total_timesteps = int(100_000 * multiplicator)\n",
    "        eval_interval = 10_000   # Increased interval since DQN learns differently\n",
    "        n_session = total_timesteps//eval_interval\n",
    "        num_eval_episodes = 5\n",
    "\n",
    "        # Training loop with periodic evaluation\n",
    "        for _ in range(0, total_timesteps, eval_interval):\n",
    "            self.model.learn(total_timesteps=eval_interval)\n",
    "            avg_reward = evaluate_model(self.model, eval_env, num_episodes=num_eval_episodes)\n",
    "            print(f\"Evaluation average reward: {avg_reward}\")\n",
    "\n",
    "    def save(self, name=\"\"):\n",
    "        self.model.save(f\"{self.model_name}_{name}_snake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-01T16:15:30.358239Z",
     "iopub.status.busy": "2023-12-01T16:15:30.357864Z",
     "iopub.status.idle": "2023-12-01T16:39:53.639849Z",
     "shell.execute_reply": "2023-12-01T16:39:53.638862Z",
     "shell.execute_reply.started": "2023-12-01T16:15:30.358207Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to save_logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorian/Documents/CODE/python/python_datascience/Reinforcement_learning/Snake_game/env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 500`, after every 7 untruncated mini-batches, there will be a truncated mini-batch of size 52\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=100 and n_envs=5)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.01     |\n",
      "|    ep_rew_mean     | -9       |\n",
      "| time/              |          |\n",
      "|    fps             | 2034     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.92       |\n",
      "|    ep_rew_mean          | -8.73      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1265       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 0          |\n",
      "|    total_timesteps      | 1000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01685164 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | -0.00183   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22.5       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 55.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.14        |\n",
      "|    ep_rew_mean          | -8.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1121        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020175237 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.00224    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.42        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 25.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.19        |\n",
      "|    ep_rew_mean          | -6.62       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1049        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018397585 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.0827     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.23        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.15       |\n",
      "|    ep_rew_mean          | -7.19      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1018       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02110986 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.0268     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.6       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 17.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75        |\n",
      "|    ep_rew_mean          | -6.19       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 994         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019316025 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.0348     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.2         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    value_loss           | 9.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.69        |\n",
      "|    ep_rew_mean          | -4.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 980         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020612922 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | -0.0295     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.19        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.7         |\n",
      "|    ep_rew_mean          | -2.03       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 971         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013733482 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.873      |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 31.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.32       |\n",
      "|    ep_rew_mean          | -1.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 974        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01045185 |\n",
      "|    clip_fraction        | 0.0814     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.81      |\n",
      "|    explained_variance   | 0.0328     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.6       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 31.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.28        |\n",
      "|    ep_rew_mean          | 1.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 978         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013312936 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.59        |\n",
      "|    ep_rew_mean          | 2.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 982         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833979 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.0491      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 10.3         |\n",
      "|    ep_rew_mean          | 3.89         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 978          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056534633 |\n",
      "|    clip_fraction        | 0.0633       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.673       |\n",
      "|    explained_variance   | 0.0274       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.8         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 61.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.8        |\n",
      "|    ep_rew_mean          | 4.88        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 975         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007391945 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 56.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.1        |\n",
      "|    ep_rew_mean          | 6.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 972         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005499067 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.048       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    value_loss           | 94.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 11.7         |\n",
      "|    ep_rew_mean          | 7.99         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 966          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054768426 |\n",
      "|    clip_fraction        | 0.068        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.031        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 47           |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    value_loss           | 88.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.1        |\n",
      "|    ep_rew_mean          | 6.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 960         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004897969 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.0553      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00984    |\n",
      "|    value_loss           | 85.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.9        |\n",
      "|    ep_rew_mean          | 7.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 955         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004328121 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.0605      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 12.8         |\n",
      "|    ep_rew_mean          | 10.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 954          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 9000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077158543 |\n",
      "|    clip_fraction        | 0.0742       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.115        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.9         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 79           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.3        |\n",
      "|    ep_rew_mean          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 957         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009743979 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.0984      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31          |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 83.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 12.7         |\n",
      "|    ep_rew_mean          | 10.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 927          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048535466 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.079        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.1         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    value_loss           | 100          |\n",
      "------------------------------------------\n",
      "Evaluation average reward: [1166.  1188.4 1197.  1122.  1120.4]\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(features_extractor_class=LinearQNet)\n",
    "\n",
    "#CnnPolicy if obs is image-like, MlpPolicy if obs is vector-like\n",
    "# model = get_model(\"DQN\", use_frame_stack=False, policy_kwargs=policy_kwargs)              \n",
    "model = ModelTrainer(\"PPO\", game_size=10, verbose=0)\n",
    "model.train(1/10)  # Train the model\n",
    "model.save(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelTrainer(\"PPO\", use_frame_stack=True, policy_kwargs=policy_kwargs)      \n",
    "model.train()  # Train the model\n",
    "model.save(\"custom_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModelTrainer(\"DQN\", use_frame_stack=False)\n",
    "# model.train()  # Train the model\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-01T13:53:22.346726Z",
     "iopub.status.idle": "2023-12-01T13:53:22.347189Z",
     "shell.execute_reply": "2023-12-01T13:53:22.346976Z",
     "shell.execute_reply.started": "2023-12-01T13:53:22.346953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! tensorboard --logdir=path_to_save_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelRender:\n",
    "    def __init__(self, name:str, use_frame_stack:bool=True, game_size:int=30, n_stack:int=4):\n",
    "        self.env = SnakeEnv(game_size=game_size)\n",
    "        path = Path().cwd() / f\"{name}.zip\" \n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Model file {path} does not exist. Please train the model first.\")\n",
    "        self.n = n_stack\n",
    "        self.use_frame_stack = use_frame_stack\n",
    "        self.name = name\n",
    "\n",
    "        if \"PPO\" in name:\n",
    "            self.model = PPO.load(name, env=get_env(use_frame_stack=use_frame_stack, game_size=game_size, n_stack=n_stack))  # Ensure the model is compatible with the environment\n",
    "        elif \"DQN\" in name:\n",
    "            self.model = DQN.load(name, env=get_env(use_frame_stack=use_frame_stack, game_size=game_size, n_stack=n_stack))  # Ensure the model is compatible with the environment\n",
    "        else:\n",
    "            raise ValueError(f\"Model {name} is not supported for rendering.\")\n",
    "\n",
    "    def render(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        self.env.render()\n",
    "        step = 0\n",
    "        while not terminated:\n",
    "            if \"PPO\" in self.name and self.use_frame_stack:\n",
    "                obs = np.concatenate([obs]*self.n).reshape((-1, 1)).flatten()\n",
    "                \n",
    "            action, _info = self.model.predict(obs, deterministic=True)\n",
    "            print(f\"Action taken: {action}\")\n",
    "            obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            step += 1\n",
    "            print(f\"Reward received: {reward}\")\n",
    "            print(\"distance to food:\", obs.take(-2))\n",
    "            print(f\"step :{step}\")\n",
    "            self.env.render()\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRender(\"PPO__snake\", game_size=10)\n",
    "model.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRender(\"PPO_N30_snake\", game_size=30)\n",
    "model.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRender(\"PPO_N30_custom_policy_snake\", game_size=30)\n",
    "model.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ModelRender(\"DQN__snake\", use_frame_stack=False, game_size=10)\n",
    "model.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
